{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Xianhui\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-class classification with Keras and TensorFlow\n",
    "https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cyret\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = load_iris()\n",
    "X, y = _.data, _.target\n",
    "print(X.shape, y.shape)\n",
    "num_classes = len(np.unique(y))\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_y)\n",
    "dummy_y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(input_dim=4, num_classes=3):\n",
    "    '''\n",
    "    set up keras neural network for multi-class\n",
    "    input_dim: number of features\n",
    "    num_classes: output layer units, for multi-class > 2\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=16, input_dim=input_dim))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dense(units=16))\n",
    "    # model.add(Activation('softmax'))\n",
    "    # model.add(Dense(units=16))\n",
    "    # model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # 3 units for 3 classes\n",
    "    adam = Adam(lr=0.001)\n",
    "    model.compile(optimizer=adam, loss=binary_crossentropy, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/150\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 1.3083 - acc: 0.5481 - val_loss: 1.4372 - val_acc: 0.4667\n",
      "Epoch 2/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 1.2483 - acc: 0.5481 - val_loss: 1.3744 - val_acc: 0.4667\n",
      "Epoch 3/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 1.1949 - acc: 0.5481 - val_loss: 1.3168 - val_acc: 0.4667\n",
      "Epoch 4/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 1.1429 - acc: 0.5481 - val_loss: 1.2669 - val_acc: 0.4667\n",
      "Epoch 5/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 1.0988 - acc: 0.5481 - val_loss: 1.2243 - val_acc: 0.4667\n",
      "Epoch 6/150\n",
      "90/90 [==============================] - 0s 105us/step - loss: 1.0641 - acc: 0.5630 - val_loss: 1.1872 - val_acc: 0.4667\n",
      "Epoch 7/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 1.0301 - acc: 0.6259 - val_loss: 1.1556 - val_acc: 0.5667\n",
      "Epoch 8/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 1.0025 - acc: 0.7000 - val_loss: 1.1277 - val_acc: 0.6000\n",
      "Epoch 9/150\n",
      "90/90 [==============================] - 0s 132us/step - loss: 0.9784 - acc: 0.7222 - val_loss: 1.1035 - val_acc: 0.6667\n",
      "Epoch 10/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.9567 - acc: 0.7444 - val_loss: 1.0813 - val_acc: 0.7333\n",
      "Epoch 11/150\n",
      "90/90 [==============================] - 0s 132us/step - loss: 0.9371 - acc: 0.7593 - val_loss: 1.0599 - val_acc: 0.7333\n",
      "Epoch 12/150\n",
      "90/90 [==============================] - 0s 132us/step - loss: 0.9169 - acc: 0.7667 - val_loss: 1.0386 - val_acc: 0.7333\n",
      "Epoch 13/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.8978 - acc: 0.7667 - val_loss: 1.0155 - val_acc: 0.7333\n",
      "Epoch 14/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.8774 - acc: 0.7667 - val_loss: 0.9918 - val_acc: 0.7333\n",
      "Epoch 15/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.8569 - acc: 0.7630 - val_loss: 0.9672 - val_acc: 0.7333\n",
      "Epoch 16/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.8359 - acc: 0.7667 - val_loss: 0.9429 - val_acc: 0.7333\n",
      "Epoch 17/150\n",
      "90/90 [==============================] - 0s 132us/step - loss: 0.8159 - acc: 0.7741 - val_loss: 0.9185 - val_acc: 0.7333\n",
      "Epoch 18/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.7950 - acc: 0.7815 - val_loss: 0.8939 - val_acc: 0.7333\n",
      "Epoch 19/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.7737 - acc: 0.7815 - val_loss: 0.8702 - val_acc: 0.7333\n",
      "Epoch 20/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.7538 - acc: 0.7778 - val_loss: 0.8484 - val_acc: 0.7333\n",
      "Epoch 21/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.7354 - acc: 0.7926 - val_loss: 0.8277 - val_acc: 0.7333\n",
      "Epoch 22/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.7198 - acc: 0.8000 - val_loss: 0.8072 - val_acc: 0.7333\n",
      "Epoch 23/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.7036 - acc: 0.8074 - val_loss: 0.7872 - val_acc: 0.7667\n",
      "Epoch 24/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.6848 - acc: 0.8148 - val_loss: 0.7677 - val_acc: 0.7333\n",
      "Epoch 25/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.6695 - acc: 0.8185 - val_loss: 0.7482 - val_acc: 0.7667\n",
      "Epoch 26/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.6514 - acc: 0.8259 - val_loss: 0.7292 - val_acc: 0.7333\n",
      "Epoch 27/150\n",
      "90/90 [==============================] - 0s 121us/step - loss: 0.6352 - acc: 0.8222 - val_loss: 0.7103 - val_acc: 0.7333\n",
      "Epoch 28/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.6189 - acc: 0.8222 - val_loss: 0.6917 - val_acc: 0.7333\n",
      "Epoch 29/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.6028 - acc: 0.8259 - val_loss: 0.6734 - val_acc: 0.7333\n",
      "Epoch 30/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.5871 - acc: 0.8259 - val_loss: 0.6552 - val_acc: 0.7333\n",
      "Epoch 31/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.5709 - acc: 0.8333 - val_loss: 0.6368 - val_acc: 0.7667\n",
      "Epoch 32/150\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.5560 - acc: 0.8444 - val_loss: 0.6177 - val_acc: 0.8000\n",
      "Epoch 33/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.5418 - acc: 0.8444 - val_loss: 0.5991 - val_acc: 0.8000\n",
      "Epoch 34/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.5251 - acc: 0.8444 - val_loss: 0.5816 - val_acc: 0.8333\n",
      "Epoch 35/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.5109 - acc: 0.8481 - val_loss: 0.5644 - val_acc: 0.8333\n",
      "Epoch 36/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.4965 - acc: 0.8556 - val_loss: 0.5479 - val_acc: 0.8333\n",
      "Epoch 37/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.4824 - acc: 0.8593 - val_loss: 0.5314 - val_acc: 0.8333\n",
      "Epoch 38/150\n",
      "90/90 [==============================] - 0s 127us/step - loss: 0.4692 - acc: 0.8593 - val_loss: 0.5152 - val_acc: 0.8333\n",
      "Epoch 39/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.4568 - acc: 0.8593 - val_loss: 0.5001 - val_acc: 0.8333\n",
      "Epoch 40/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.4434 - acc: 0.8593 - val_loss: 0.4859 - val_acc: 0.8333\n",
      "Epoch 41/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.4320 - acc: 0.8593 - val_loss: 0.4726 - val_acc: 0.8333\n",
      "Epoch 42/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.4224 - acc: 0.8667 - val_loss: 0.4597 - val_acc: 0.8667\n",
      "Epoch 43/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.4117 - acc: 0.8667 - val_loss: 0.4487 - val_acc: 0.8667\n",
      "Epoch 44/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.4033 - acc: 0.8667 - val_loss: 0.4381 - val_acc: 0.8667\n",
      "Epoch 45/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.3944 - acc: 0.8741 - val_loss: 0.4285 - val_acc: 0.8333\n",
      "Epoch 46/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.3872 - acc: 0.8741 - val_loss: 0.4196 - val_acc: 0.8333\n",
      "Epoch 47/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.3801 - acc: 0.8704 - val_loss: 0.4125 - val_acc: 0.8333\n",
      "Epoch 48/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.3743 - acc: 0.8630 - val_loss: 0.4053 - val_acc: 0.8333\n",
      "Epoch 49/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.3683 - acc: 0.8593 - val_loss: 0.3991 - val_acc: 0.8333\n",
      "Epoch 50/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.3627 - acc: 0.8556 - val_loss: 0.3936 - val_acc: 0.8333\n",
      "Epoch 51/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.3576 - acc: 0.8593 - val_loss: 0.3880 - val_acc: 0.8333\n",
      "Epoch 52/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.3530 - acc: 0.8667 - val_loss: 0.3827 - val_acc: 0.8333\n",
      "Epoch 53/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.3482 - acc: 0.8704 - val_loss: 0.3786 - val_acc: 0.8333\n",
      "Epoch 54/150\n",
      "90/90 [==============================] - 0s 121us/step - loss: 0.3439 - acc: 0.8741 - val_loss: 0.3746 - val_acc: 0.8667\n",
      "Epoch 55/150\n",
      "90/90 [==============================] - 0s 121us/step - loss: 0.3399 - acc: 0.8741 - val_loss: 0.3702 - val_acc: 0.8667\n",
      "Epoch 56/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.3359 - acc: 0.8741 - val_loss: 0.3672 - val_acc: 0.8667\n",
      "Epoch 57/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.3322 - acc: 0.8741 - val_loss: 0.3631 - val_acc: 0.8667\n",
      "Epoch 58/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.3287 - acc: 0.8741 - val_loss: 0.3601 - val_acc: 0.8667\n",
      "Epoch 59/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.3251 - acc: 0.8741 - val_loss: 0.3562 - val_acc: 0.8667\n",
      "Epoch 60/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.3217 - acc: 0.8741 - val_loss: 0.3527 - val_acc: 0.8667\n",
      "Epoch 61/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.3185 - acc: 0.8741 - val_loss: 0.3485 - val_acc: 0.8667\n",
      "Epoch 62/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.3158 - acc: 0.8741 - val_loss: 0.3443 - val_acc: 0.8667\n",
      "Epoch 63/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.3122 - acc: 0.8815 - val_loss: 0.3411 - val_acc: 0.8667\n",
      "Epoch 64/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.3094 - acc: 0.8815 - val_loss: 0.3377 - val_acc: 0.8667\n",
      "Epoch 65/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.3066 - acc: 0.8815 - val_loss: 0.3344 - val_acc: 0.8667\n",
      "Epoch 66/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.3043 - acc: 0.8852 - val_loss: 0.3314 - val_acc: 0.8667\n",
      "Epoch 67/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.3011 - acc: 0.8889 - val_loss: 0.3286 - val_acc: 0.8667\n",
      "Epoch 68/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2984 - acc: 0.8889 - val_loss: 0.3263 - val_acc: 0.8667\n",
      "Epoch 69/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.2958 - acc: 0.8963 - val_loss: 0.3240 - val_acc: 0.8667\n",
      "Epoch 70/150\n",
      "90/90 [==============================] - 0s 121us/step - loss: 0.2934 - acc: 0.9000 - val_loss: 0.3216 - val_acc: 0.9333\n",
      "Epoch 71/150\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.2910 - acc: 0.9074 - val_loss: 0.3196 - val_acc: 0.9333\n",
      "Epoch 72/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.2889 - acc: 0.9074 - val_loss: 0.3170 - val_acc: 0.9333\n",
      "Epoch 73/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.2865 - acc: 0.9148 - val_loss: 0.3145 - val_acc: 0.9333\n",
      "Epoch 74/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2844 - acc: 0.9148 - val_loss: 0.3120 - val_acc: 0.9333\n",
      "Epoch 75/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2823 - acc: 0.9185 - val_loss: 0.3094 - val_acc: 0.9667\n",
      "Epoch 76/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2803 - acc: 0.9222 - val_loss: 0.3065 - val_acc: 0.9667\n",
      "Epoch 77/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2782 - acc: 0.9333 - val_loss: 0.3039 - val_acc: 0.9667\n",
      "Epoch 78/150\n",
      "90/90 [==============================] - 0s 66us/step - loss: 0.2765 - acc: 0.9296 - val_loss: 0.3020 - val_acc: 0.9667\n",
      "Epoch 79/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2746 - acc: 0.9333 - val_loss: 0.3001 - val_acc: 0.9667\n",
      "Epoch 80/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2726 - acc: 0.9333 - val_loss: 0.2983 - val_acc: 0.9667\n",
      "Epoch 81/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2710 - acc: 0.9333 - val_loss: 0.2966 - val_acc: 0.9667\n",
      "Epoch 82/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2690 - acc: 0.9296 - val_loss: 0.2947 - val_acc: 0.9667\n",
      "Epoch 83/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2675 - acc: 0.9333 - val_loss: 0.2929 - val_acc: 0.9667\n",
      "Epoch 84/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2657 - acc: 0.9333 - val_loss: 0.2914 - val_acc: 0.9667\n",
      "Epoch 85/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2642 - acc: 0.9407 - val_loss: 0.2895 - val_acc: 0.9667\n",
      "Epoch 86/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.2625 - acc: 0.9444 - val_loss: 0.2879 - val_acc: 0.9667\n",
      "Epoch 87/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2610 - acc: 0.9444 - val_loss: 0.2864 - val_acc: 0.9667\n",
      "Epoch 88/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2593 - acc: 0.9370 - val_loss: 0.2846 - val_acc: 0.9667\n",
      "Epoch 89/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2577 - acc: 0.9407 - val_loss: 0.2828 - val_acc: 0.9667\n",
      "Epoch 90/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2564 - acc: 0.9407 - val_loss: 0.2810 - val_acc: 0.9667\n",
      "Epoch 91/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2549 - acc: 0.9519 - val_loss: 0.2796 - val_acc: 0.9667\n",
      "Epoch 92/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2533 - acc: 0.9519 - val_loss: 0.2782 - val_acc: 0.9667\n",
      "Epoch 93/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2520 - acc: 0.9556 - val_loss: 0.2766 - val_acc: 1.0000\n",
      "Epoch 94/150\n",
      "90/90 [==============================] - 0s 132us/step - loss: 0.2506 - acc: 0.9630 - val_loss: 0.2751 - val_acc: 1.0000\n",
      "Epoch 95/150\n",
      "90/90 [==============================] - 0s 171us/step - loss: 0.2494 - acc: 0.9593 - val_loss: 0.2738 - val_acc: 0.9667\n",
      "Epoch 96/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2479 - acc: 0.9481 - val_loss: 0.2722 - val_acc: 0.9667\n",
      "Epoch 97/150\n",
      "90/90 [==============================] - 0s 165us/step - loss: 0.2466 - acc: 0.9481 - val_loss: 0.2705 - val_acc: 1.0000\n",
      "Epoch 98/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2453 - acc: 0.9630 - val_loss: 0.2691 - val_acc: 1.0000\n",
      "Epoch 99/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.2441 - acc: 0.9630 - val_loss: 0.2677 - val_acc: 1.0000\n",
      "Epoch 100/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2425 - acc: 0.9630 - val_loss: 0.2663 - val_acc: 1.0000\n",
      "Epoch 101/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.2412 - acc: 0.9667 - val_loss: 0.2650 - val_acc: 1.0000\n",
      "Epoch 102/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2401 - acc: 0.9630 - val_loss: 0.2637 - val_acc: 1.0000\n",
      "Epoch 103/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2388 - acc: 0.9593 - val_loss: 0.2624 - val_acc: 1.0000\n",
      "Epoch 104/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2375 - acc: 0.9593 - val_loss: 0.2610 - val_acc: 1.0000\n",
      "Epoch 105/150\n",
      "90/90 [==============================] - 0s 231us/step - loss: 0.2366 - acc: 0.9630 - val_loss: 0.2597 - val_acc: 1.0000\n",
      "Epoch 106/150\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.2352 - acc: 0.9630 - val_loss: 0.2584 - val_acc: 1.0000\n",
      "Epoch 107/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2340 - acc: 0.9630 - val_loss: 0.2571 - val_acc: 1.0000\n",
      "Epoch 108/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2329 - acc: 0.9630 - val_loss: 0.2558 - val_acc: 1.0000\n",
      "Epoch 109/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.2318 - acc: 0.9667 - val_loss: 0.2546 - val_acc: 1.0000\n",
      "Epoch 110/150\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.2305 - acc: 0.9630 - val_loss: 0.2533 - val_acc: 1.0000\n",
      "Epoch 111/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2297 - acc: 0.9593 - val_loss: 0.2522 - val_acc: 1.0000\n",
      "Epoch 112/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.2285 - acc: 0.9667 - val_loss: 0.2510 - val_acc: 1.0000\n",
      "Epoch 113/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2272 - acc: 0.9630 - val_loss: 0.2498 - val_acc: 1.0000\n",
      "Epoch 114/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2264 - acc: 0.9630 - val_loss: 0.2486 - val_acc: 1.0000\n",
      "Epoch 115/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2252 - acc: 0.9630 - val_loss: 0.2474 - val_acc: 1.0000\n",
      "Epoch 116/150\n",
      "90/90 [==============================] - 0s 105us/step - loss: 0.2240 - acc: 0.9630 - val_loss: 0.2462 - val_acc: 1.0000\n",
      "Epoch 117/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2230 - acc: 0.9667 - val_loss: 0.2451 - val_acc: 1.0000\n",
      "Epoch 118/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2219 - acc: 0.9630 - val_loss: 0.2439 - val_acc: 1.0000\n",
      "Epoch 119/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2213 - acc: 0.9593 - val_loss: 0.2427 - val_acc: 1.0000\n",
      "Epoch 120/150\n",
      "90/90 [==============================] - 0s 198us/step - loss: 0.2200 - acc: 0.9593 - val_loss: 0.2417 - val_acc: 1.0000\n",
      "Epoch 121/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2191 - acc: 0.9630 - val_loss: 0.2405 - val_acc: 1.0000\n",
      "Epoch 122/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2180 - acc: 0.9630 - val_loss: 0.2394 - val_acc: 1.0000\n",
      "Epoch 123/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2170 - acc: 0.9593 - val_loss: 0.2381 - val_acc: 1.0000\n",
      "Epoch 124/150\n",
      "90/90 [==============================] - 0s 116us/step - loss: 0.2159 - acc: 0.9630 - val_loss: 0.2369 - val_acc: 1.0000\n",
      "Epoch 125/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2151 - acc: 0.9593 - val_loss: 0.2358 - val_acc: 1.0000\n",
      "Epoch 126/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.2140 - acc: 0.9593 - val_loss: 0.2346 - val_acc: 1.0000\n",
      "Epoch 127/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2131 - acc: 0.9593 - val_loss: 0.2336 - val_acc: 1.0000\n",
      "Epoch 128/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.2122 - acc: 0.9630 - val_loss: 0.2326 - val_acc: 1.0000\n",
      "Epoch 129/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.2112 - acc: 0.9593 - val_loss: 0.2316 - val_acc: 1.0000\n",
      "Epoch 130/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2102 - acc: 0.9593 - val_loss: 0.2305 - val_acc: 1.0000\n",
      "Epoch 131/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2093 - acc: 0.9593 - val_loss: 0.2293 - val_acc: 1.0000\n",
      "Epoch 132/150\n",
      "90/90 [==============================] - 0s 110us/step - loss: 0.2083 - acc: 0.9630 - val_loss: 0.2283 - val_acc: 1.0000\n",
      "Epoch 133/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2075 - acc: 0.9630 - val_loss: 0.2272 - val_acc: 1.0000\n",
      "Epoch 134/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2066 - acc: 0.9630 - val_loss: 0.2262 - val_acc: 1.0000\n",
      "Epoch 135/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2056 - acc: 0.9630 - val_loss: 0.2252 - val_acc: 1.0000\n",
      "Epoch 136/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2048 - acc: 0.9630 - val_loss: 0.2241 - val_acc: 1.0000\n",
      "Epoch 137/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.2039 - acc: 0.9630 - val_loss: 0.2232 - val_acc: 1.0000\n",
      "Epoch 138/150\n",
      "90/90 [==============================] - 0s 94us/step - loss: 0.2030 - acc: 0.9593 - val_loss: 0.2222 - val_acc: 1.0000\n",
      "Epoch 139/150\n",
      "90/90 [==============================] - 0s 83us/step - loss: 0.2021 - acc: 0.9593 - val_loss: 0.2212 - val_acc: 1.0000\n",
      "Epoch 140/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.2013 - acc: 0.9630 - val_loss: 0.2201 - val_acc: 1.0000\n",
      "Epoch 141/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.2004 - acc: 0.9630 - val_loss: 0.2190 - val_acc: 1.0000\n",
      "Epoch 142/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.1996 - acc: 0.9630 - val_loss: 0.2180 - val_acc: 1.0000\n",
      "Epoch 143/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.1988 - acc: 0.9630 - val_loss: 0.2170 - val_acc: 1.0000\n",
      "Epoch 144/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.1978 - acc: 0.9630 - val_loss: 0.2162 - val_acc: 1.0000\n",
      "Epoch 145/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.1969 - acc: 0.9593 - val_loss: 0.2152 - val_acc: 1.0000\n",
      "Epoch 146/150\n",
      "90/90 [==============================] - 0s 99us/step - loss: 0.1969 - acc: 0.9593 - val_loss: 0.2142 - val_acc: 1.0000\n",
      "Epoch 147/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.1954 - acc: 0.9630 - val_loss: 0.2134 - val_acc: 1.0000\n",
      "Epoch 148/150\n",
      "90/90 [==============================] - 0s 72us/step - loss: 0.1953 - acc: 0.9667 - val_loss: 0.2127 - val_acc: 1.0000\n",
      "Epoch 149/150\n",
      "90/90 [==============================] - 0s 88us/step - loss: 0.1942 - acc: 0.9630 - val_loss: 0.2114 - val_acc: 1.0000\n",
      "Epoch 150/150\n",
      "90/90 [==============================] - 0s 77us/step - loss: 0.1928 - acc: 0.9630 - val_loss: 0.2104 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b73325da58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = base_model()\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.03754101, 0.7026595 , 0.25979954],\n",
       "        [0.984421  , 0.01389271, 0.00168624],\n",
       "        [0.00346922, 0.1273421 , 0.8691887 ],\n",
       "        [0.03193707, 0.5803498 , 0.38771313],\n",
       "        [0.02681728, 0.632687  , 0.34049574]], dtype=float32),\n",
       " array([[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)[:5], y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## accuracy of keras model\n",
    "np.sum(np.array(model.predict(X_train) > 0.5).astype(float) == y_train) / y_train.shape[0] / y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(model.predict(X_test) > 0.5).astype(float) == y_test) / y_test.shape[0] / y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 2.0960 - acc: 0.5600\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 1.9287 - acc: 0.5600\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 1.7702 - acc: 0.5600\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 0s 139us/step - loss: 1.6132 - acc: 0.5600\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 1.4676 - acc: 0.5600\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 0s 159us/step - loss: 1.3314 - acc: 0.5600\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 1.2013 - acc: 0.5600\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 0s 109us/step - loss: 1.0934 - acc: 0.5600\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.9931 - acc: 0.5600\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 0s 119us/step - loss: 0.9155 - acc: 0.5600\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 0s 119us/step - loss: 0.8532 - acc: 0.5567\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 0.8150 - acc: 0.4967\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.7895 - acc: 0.4867\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.7727 - acc: 0.5033\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 0.7587 - acc: 0.4833\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.7482 - acc: 0.4767\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.7389 - acc: 0.4700\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.7285 - acc: 0.4633\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.7190 - acc: 0.4633\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.7093 - acc: 0.4633\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 0s 144us/step - loss: 0.7005 - acc: 0.4667\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.6920 - acc: 0.4733\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.6823 - acc: 0.4900\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 0s 129us/step - loss: 0.6728 - acc: 0.5067\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.6637 - acc: 0.5467\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.6553 - acc: 0.5633\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.6462 - acc: 0.5933\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 0s 129us/step - loss: 0.6374 - acc: 0.6200\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 0s 124us/step - loss: 0.6286 - acc: 0.6333\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.6208 - acc: 0.6433\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 0.6130 - acc: 0.6433\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.6061 - acc: 0.6467\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.5987 - acc: 0.6533\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.5929 - acc: 0.6567\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.5870 - acc: 0.6633\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.5809 - acc: 0.6633\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.5749 - acc: 0.6667\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.5695 - acc: 0.6667\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.5638 - acc: 0.6667\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.5587 - acc: 0.6667\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.5535 - acc: 0.6667\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.5485 - acc: 0.6700\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 0s 109us/step - loss: 0.5438 - acc: 0.6700\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.5389 - acc: 0.6700\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.5341 - acc: 0.6700\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.5295 - acc: 0.6700\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 0s 154us/step - loss: 0.5251 - acc: 0.6700\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.5201 - acc: 0.6700\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.5158 - acc: 0.6767\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.5119 - acc: 0.7100\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.5080 - acc: 0.7367\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.5043 - acc: 0.7600\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.5002 - acc: 0.7633\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4971 - acc: 0.7567\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.4934 - acc: 0.7567\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4898 - acc: 0.7600\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 0s 134us/step - loss: 0.4863 - acc: 0.7600\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4827 - acc: 0.7767\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.4785 - acc: 0.8333\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.4767 - acc: 0.8500\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4734 - acc: 0.8567\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4698 - acc: 0.8500\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4663 - acc: 0.8567\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4627 - acc: 0.8633\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4597 - acc: 0.8600\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.4560 - acc: 0.8533\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.4528 - acc: 0.8433\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 0s 134us/step - loss: 0.4496 - acc: 0.8533\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.4468 - acc: 0.8667\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4441 - acc: 0.8700\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.4410 - acc: 0.8633\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4383 - acc: 0.8633\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4354 - acc: 0.8633\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4327 - acc: 0.8667\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4302 - acc: 0.8600\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.4276 - acc: 0.8600\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4251 - acc: 0.8667\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.4223 - acc: 0.8700\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.4198 - acc: 0.8700\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.4175 - acc: 0.8700\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 0s 149us/step - loss: 0.4150 - acc: 0.8700\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.4126 - acc: 0.8700\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.4103 - acc: 0.8700\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.4088 - acc: 0.8700\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.4057 - acc: 0.8633\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.4035 - acc: 0.8633\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.4014 - acc: 0.8567\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3992 - acc: 0.8533\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3975 - acc: 0.8500\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3956 - acc: 0.8500\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 0.3936 - acc: 0.8500\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 0.3915 - acc: 0.8500\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3896 - acc: 0.8500\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3880 - acc: 0.8400\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3867 - acc: 0.8333\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3853 - acc: 0.8300\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3837 - acc: 0.8300\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.3814 - acc: 0.8300\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.3782 - acc: 0.8433\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3773 - acc: 0.8600\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3738 - acc: 0.8733\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3732 - acc: 0.8667\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3722 - acc: 0.8667\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 0s 119us/step - loss: 0.3701 - acc: 0.8667\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3674 - acc: 0.8733\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3661 - acc: 0.8567\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.3651 - acc: 0.8467\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3635 - acc: 0.8333\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3621 - acc: 0.8300\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.3600 - acc: 0.8400\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3583 - acc: 0.8467\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.3568 - acc: 0.8500\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3555 - acc: 0.8400\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3542 - acc: 0.8367\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.3529 - acc: 0.8367\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.3508 - acc: 0.8500\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3502 - acc: 0.8567\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.3479 - acc: 0.8767\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.3475 - acc: 0.8767\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3463 - acc: 0.8767\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.3444 - acc: 0.8767\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 0s 74us/step - loss: 0.3428 - acc: 0.8733\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3410 - acc: 0.8700\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.3391 - acc: 0.8700\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 0s 109us/step - loss: 0.3378 - acc: 0.8633\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 0s 109us/step - loss: 0.3367 - acc: 0.8500\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.3355 - acc: 0.8500\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3343 - acc: 0.8500\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.3328 - acc: 0.8500\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.3313 - acc: 0.8533\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 0s 119us/step - loss: 0.3300 - acc: 0.8600\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 0s 109us/step - loss: 0.3286 - acc: 0.8633\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3274 - acc: 0.8633\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3263 - acc: 0.8633\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.3251 - acc: 0.8633\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3238 - acc: 0.8600\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3230 - acc: 0.8500\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 0s 104us/step - loss: 0.3225 - acc: 0.8467\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3216 - acc: 0.8433\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 0s 99us/step - loss: 0.3209 - acc: 0.8400\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 0s 109us/step - loss: 0.3199 - acc: 0.8433\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 0s 114us/step - loss: 0.3180 - acc: 0.8433\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 0s 124us/step - loss: 0.3166 - acc: 0.8433\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 0s 179us/step - loss: 0.3149 - acc: 0.8533\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 0s 84us/step - loss: 0.3137 - acc: 0.8633\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.3123 - acc: 0.8633\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3116 - acc: 0.8533\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3108 - acc: 0.8500\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 0s 119us/step - loss: 0.3096 - acc: 0.8500\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.3083 - acc: 0.8533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b733635c18>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "estimator = KerasClassifier(build_fn=base_model, epochs=150, batch_size=32, verbose=1)\n",
    "estimator.fit(X_train, y_train)\n",
    "# results = cross_val_score(model, X, dummy_y, cv=kfold, scoring=accuracy_score)\n",
    "# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 301us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, estimator.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'accuracy score {accuracy_score(y_train, estimator.predict(X_train)).round(2)}')\n",
    "# print(f'accuracy score {accuracy_score(y_test, estimator.predict(X_test)).round(2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 84us/step\n",
      "50/50 [==============================] - 0s 69us/step\n",
      "train loss+score: [0.19410236120224, 0.9666666412353515],\n",
      " test loss+score: [0.16639681816101073, 0.9733333539962769]\n"
     ]
    }
   ],
   "source": [
    "print(f'train loss+score: {model.evaluate(X_train, y_train)},\\n test loss+score: {model.evaluate(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
